{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators and generators.py\n",
    "Now that you've retrieved the context needed for your query, the only step left is to prompt your LLM with the retrieved context appropriately. In a production setting, you may spend some time optimizing your prompt with techniques like Chain of Thought or using prompt compilation with DSPy. Generally, the better the inference performance you are trying to get out of a smaller model, the more optimization you will have to do to achieve acceptable performance.\n",
    "\n",
    "The following cheat code should work if you're run your indexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Naive RAG represents the earliest methodology in RAG research, following a traditional \"Retrieve-Read\" framework involving indexing, retrieval, and generation processes. Advanced RAG and Modular RAG were developed in response to the limitations of Naive RAG, offering more sophisticated and adaptable approaches to information retrieval and generation tasks. Advanced RAG introduces enhancements like unbounded memory pools, routing mechanisms, and task adapter modules to improve the quality, relevance, and flexibility of retrieved information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cheat_code.common_components.vectorizers import Vectorizer\n",
    "from cheat_code.retrievers import NaiveRetriever\n",
    "from cheat_code.generators import NaiveGenerator\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "retriever = NaiveRetriever(vectorizer)\n",
    "generator = NaiveGenerator()\n",
    "\n",
    "query = \"What's the difference between Naive RAG and Advanced RAG?\"\n",
    "retrieved_context = retriever.retrieve(query)\n",
    "completion = generator.get_completion(query, retrieved_context)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Write a system prompt and prompt for the LLM that combines your query with the retrieved context\n",
    "Modify the code in `./workshop_code/generators.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are several types of adversarial attacks that can be launched against Language Models (LLMs) such as GPT-3. Some of the common types of adversarial attacks include:\n",
       "\n",
       "1. Textual Adversarial Attacks: In this type of attack, the adversary manipulates the input text in such a way that it appears benign to a human reader but can cause the LLM to generate incorrect or malicious outputs. This can be achieved through techniques like adding or removing words, changing the order of words, or introducing subtle changes to the text.\n",
       "\n",
       "2. Poisoning Attacks: In poisoning attacks, the adversary injects malicious or misleading data into the training data used to train the LLM. This can lead to the model learning incorrect patterns and producing biased or inaccurate outputs when presented with certain inputs.\n",
       "\n",
       "3. Evasion Attacks: Evasion attacks aim to deceive the LLM by crafting inputs that exploit vulnerabilities in the model's architecture or training data. By carefully designing input sequences, the adversary can trick the model into generating incorrect or unintended outputs.\n",
       "\n",
       "4. Model Inversion Attacks: In model inversion attacks, the adversary tries to reverse-engineer the LLM to extract sensitive information from the model's internal representations. This can pose a serious privacy risk, especially if the model has been"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from workshop_code.common_components.vectorizers import Vectorizer\n",
    "from workshop_code.retrievers import NaiveRetriever\n",
    "from workshop_code.generators import NaiveGenerator\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "retriever = NaiveRetriever(vectorizer)\n",
    "generator = NaiveGenerator()\n",
    "\n",
    "query = \"Boop\"\n",
    "retrieved_context = retriever.retrieve(query)\n",
    "print(\"Retrieved Context:\", retrieved_context)\n",
    "completion = generator.get_completion(query, retrieved_context)\n",
    "display_md(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
