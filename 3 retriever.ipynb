{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers & retrievers.py\n",
    "In order to retrieve relevant chunks of text from Weaviate, we must first vectorize our query using the same model we created our text chunk embeddings with. We then give that query vector to Weaviate. Weaviate uses the query vector to perform a cosine similarity search for the most similar text chunks.\n",
    "\n",
    "If you've already run your indexer, the below code should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/Development/rag-mini-bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliance on pre-training data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional pro\n",
      "\n",
      "====================================\n",
      "\n",
      "include RAG-centric services. Weaviate’s Verba111111https://github.com/weaviate/Verbais designed for personal assistant applications, while Amazon’s Kendra121212https://aws.amazon.com/cn/kendra/offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. In the development of RAG technology, there is a clear trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwor\n"
     ]
    }
   ],
   "source": [
    "from cheat_code.common_components.vectorizers import Vectorizer\n",
    "from cheat_code.common_components.wcs_client_adapter import WcsClientAdapter\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "wcs_client_adapter = WcsClientAdapter()\n",
    "\n",
    "query = \"How is Naive RAG different from Advanced RAG?\"\n",
    "query_vector = vectorizer.vectorize_query(query)\n",
    "retrieved_chunks_list = wcs_client_adapter.retrieve(query_vector, k=2)\n",
    "\n",
    "print(retrieved_chunks_list[0][0:1000])\n",
    "print(\"\\n====================================\\n\")\n",
    "print(retrieved_chunks_list[1][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever task #1: implement vectorize_query() in workshop_code/common_components/vectorizers.py\n",
    "This should be easy because it's essentially a simpler version of `vectorize_text_chunks()`, which you've already completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliance on pre-training data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional pro\n",
      "\n",
      "====================================\n",
      "\n",
      "include RAG-centric services. Weaviate’s Verba111111https://github.com/weaviate/Verbais designed for personal assistant applications, while Amazon’s Kendra121212https://aws.amazon.com/cn/kendra/offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. In the development of RAG technology, there is a clear trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwor\n"
     ]
    }
   ],
   "source": [
    "from workshop_code.common_components.vectorizers import Vectorizer\n",
    "from workshop_code.common_components.wcs_client_adapter import WcsClientAdapter\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "wcs_client_adapter = WcsClientAdapter()\n",
    "\n",
    "query = \"How is Naive RAG different from Advanced RAG?\"\n",
    "query_vector = vectorizer.vectorize_query(query)\n",
    "retrieved_chunks_list = wcs_client_adapter.retrieve(query_vector, k=2)\n",
    "\n",
    "print(retrieved_chunks_list[0][0:1000])\n",
    "print(\"\\n====================================\\n\")\n",
    "print(retrieved_chunks_list[1][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever task #2: understand `retrievers.py`\n",
    "Look over the code of `retrievers.py`. If something doesn't make sense, ask one of us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
