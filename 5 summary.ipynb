{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/Development/rag-mini-bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from workshop_code.agents import NaiveQaRagAgent\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "You can combine the indexer, retriever, and generator into a convenient abstracted interface like in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are several types of adversarial attacks that can be launched against Language Model Models (LLMs). Some of the common types include:\n",
       "\n",
       "1. Text-based Adversarial Attacks: In this type of attack, the adversary manipulates the input text to the LLM in such a way that it causes the model to produce incorrect or unintended outputs. This can be achieved through techniques such as adding or modifying words, inserting special characters, or using synonyms to deceive the model.\n",
       "\n",
       "2. Poisoning Attacks: In poisoning attacks, the adversary injects malicious data into the training dataset of the LLM in order to manipulate its behavior. By introducing biased or misleading examples during training, the adversary can influence the model's predictions and make it more vulnerable to attacks.\n",
       "\n",
       "3. Evasion Attacks: Evasion attacks aim to deceive the LLM by crafting input samples that are specifically designed to evade detection or classification by the model. This can be done by exploiting vulnerabilities in the model's architecture or by leveraging weaknesses in the training data.\n",
       "\n",
       "4. Model Inversion Attacks: In model inversion attacks, the adversary tries to reverse-engineer the LLM in order to extract sensitive information from it. By providing carefully crafted input samples and observing the model's outputs, the adversary can infer details about the model's"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_uri = \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "qa_agent = NaiveQaRagAgent()\n",
    "\n",
    "qa_agent.index(doc_uri)\n",
    "\n",
    "question = \"What are different types of Adversarial Attacks against LLMs?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: inspect the code\n",
    "Look at the code in `./cheat_code/agents.py`. If anything doesn't make sense, let one of us know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
